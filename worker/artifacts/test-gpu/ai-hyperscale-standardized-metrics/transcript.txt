 As you see the progress, we will use the 0.5 version, which focuses on the debunk, okay, like especially CPU debunk features across x86 and ARM, there is a lot of variances and all we want to standardize, okay, and go a lot more depth into type of debunk features, okay.  And we published a 0.7 version, more focus on the 0.7 version, I will share some details on the, what are the diagnostic improvements we added into the 0.7 version.  Like, you saw the scenario, right, when you see the rack scale, like the number of devices, suppliers, which goes into the rack scale, either accelerator, memory,  Vendor, network, even the hardware type, okay, each vendor provides say, diagnostic tools, even if some vendors have both GPU, CPU and other memory,  what we observed is that each vendor across the vendors there is a lot of variance, how they release the tools, and then as I said, we need to integrate, and we need to test it, and we need to work with them internally,  how to understand that is one of the biggest pain point what kind of physical interfaces  delivering what api can provide especially on the like you know microsoft  most we use windows now we are moving to a lot of work on linux like we want to standardize our  linux all the defendants vendors can focus like take a particular linux candle watch name of life  linux also multiple distributions but as it focus on the kernel and test those okay and  that's where we're focusing on like how do we package all the things into one thing right  we'll talk a little more details of the diagnostic recipe like generally when you provide the  diagnostics generally like you provide a documentation i run this tool this way  it's all manual techniques and  also sometimes like when we see the free tissue you reach out to the vendors they say oh run this  way this is the different recipe you need to do maybe the automatic meat errors and all that right  like that is the other problem we have and the last one is about the repair actions is mostly  focused i would say is on what are the two layers  what are the silicon is observing rather thinking about the health center technician  right with the complexity like you see liquid cooling and all that how do we replace the  one part is very very hard now with the complexity right you need to think to  repair actions in the point of service technician or like firmware updates okay you need to show the  results into action oriented not on the what you're observing okay and for this like uh  we spent some time okay even though it's part of cpu like we we made these requirements in  generally which can be applied for cpus accelerators pci devices okay like we use that platform and the  cpu works team to publish the spec but we're going to leverage this across all other components right  and the second thing is about the tools requirement right we want  same tools to work in manufacturing development phase and production  like that is the other problem we observed right you get some tools or recipes are different  and we want to standardize across the load life cycle  and if you see in the picture right when you you put in the five steps of the main  pain points one is when you develop the tools other one is when you test it the tools third is like  when you integrate it into hyper scalar or enterprise environment right and then you  execute it okay what problems we face then we are taking actions these are the major areas for the  tools uh like we are providing the requirements across like uh what kind of environment like  all icoscalers are recommending uh suppliers to develop the tools and dependencies  let's say like you need you need some special Wijzena code through driver problem or some  things that put those into package right and the second advantage is like the hyper scalability  used to work they used to test this one now with the standardization because  all are saying similar things a lot of what supplies could can go and test that  against those requirements then we can easily integrate uh and we can leverage we don't need to  to duplicate that effort testing when supplier testing because of  standardization right they can share the results we do minimum test right that  will help us save on resources the thing of integration becomes easy and an  integration especially when you see errors in the link problems and all like  whenever we reach out to different suppliers or link you need to run different tools  the recipe is different more change from somewhere from their settings there's a  lot of knowledge is shared across meetings requirements that say like  Rajat provided a talk this morning how do we do recipe generation by tool  right there's a open source contribution coming up like you can use that tool to  generate the recipes optimize it okay next thing about action you want to focus  on the what can be done as a  video  looking at the silicon point of view most silicon point of your results you  can get it from like say vast elementary what logs and all right there is a lot of  requirements specified in order what management what to do those are the  things I will give it to Jitsel from a deeper perspective we have an industry state  city  based on the  data or currentigm and we use the data to utilize this and essentially we  um take some neural networks and we particularly would encourage the  digital system to go into эту that solution function right in the social  environment we would want to adopt in particular a little smaller scale and  ultimately we would want to separate the data we provide education or where we  do also choose it you don't know data this version my IDs or scale u Speed and  the data I know features between generations of μαavy supply so what way  really would like to go is where we have people actually have a generally big worth  I'm going to do think about more macro-level technologies more up� suspensions  we are monitoring the failures happen and then in cases where we need additional information  we can configure and collect further information so what where we really would like to be is at  the connection stand where failures happen we are collecting the logs and then we act on them  so the common collection mechanism that we would like to hit would be crash terms where when the  CPU or machine fails we have the crash term the crash term gives us very clear action signals and  then we act on them now that's going to be the predominant part of how we work but in rare cases  where a small subset of the failures we need additional information so that we can debug  further now normally this happens during NPI phase where you're in the development cycle  but many a times after your production you do have to go into some kind of debug cycle  so we would like to have that capability also available and this debug cycle mainly requires  additional information which crash terms normally don't provide which could be extended register  dumps or trace dumps it could also be a state machine of how the machine is so state dumps or  second dumps we'll talk more into how these differ so in a crash term scenario what you have is a  fatal error that happens in the GPU or the CPU or a switch link etc and and this cost  you a lot of time so we would like to have that capability also available and this debug cycle  mainly requires additional information which crash terms normally don't provide which could be extended  register dumps or trace dumps or trace dumps this is the crash so that that becomes the trigger event  and based on the trigger event you then go collect the type of the failure and and based on the  failure that you see you then assert whether a certain where it is a normal failure or it's  a failure that kind of you know has to have an action upon and and in some cases you probably  need an analysis to it so some some of them are pretty straightforward you know hey if I  see this dump if I see the signal this is what I need to do but in many cases  you will need a pass or an analyzer as we call them where the crash dump is then analyzed by the  supplier provided analysis tool which then allows us to take the RAS action and and kind of indicates  what should be the next step that we should do and in certain cases we would need assistance  from the vendor when the crash dump analysis probably needs further information but that's  kind of the intern like the more detailed part of it so this whole flow happens automatically and it is a kind of a  that happens on fated failures and it happens on demand now the requirements  are obviously we don't have any customer data here it has to be processed across  the reset because crash time does cost the machine to reset and then you want  to make sure that this is usable across not just the current product but it's  kind of standardized so you can use it across the development phase and in  production and across multiple generations  SEPA is a format that we are all familiar with through the OCP efforts from the  fork management work stream and that's what we want to align with from a deeper  perspective now we get kind of going into that exceptional failure mode where  this is a smaller subset of failures that we see where we want to collect the  additional information this would require configuring or unlocking the  system in some cases class pick as we call it this happens on demand and and  this the requirements for that would be  a  having scan dumps trace dumps or additional state machine dumps taken we  would like this would be in the SEPA format and it would use the same  software interface interface as the crash dump so between these two the  functional mode would be the same it's just that there's additional data that  is provided which allows us to do the deeper and then we would like to have  this in production so it makes deeper get scaled pretty useful and easier with  this and then a near hopper for the GPU  hello  if you about som multipropolis here about standardization efforts diagnostics  and  I'm gonna focus on GPS orTIME, but by which means can I actually export a JSON material to the GPU RZ herself because I am currently handling the same system did you know anything about some  so my colleagues here in the US about standardization efforts diaght nokушen and we can show you how to do things forcefully together with that we can't communicate with methods like proxities like datarep Tim I don't wanna say much about facts because I don't want to be that  CPU RAS has been covered in other sections, so I'm not going to focus on that one.  It will be just the GPU side.  So from GPU RAS point of view, we have published our version 1.0 spec last year, September.  The focus has been on RAS metrics and what should be the targets around them.  So essentially, the main point here is, given the AI era, we have now new kinds of workloads.  We have training, we have referencing. They can be run concurrently on multiple GPUs.  So things are different. So the traditional RAS metrics would not just be applicable as is in this new AI domain.  So what we looked at is, with the conferencing thing in mind, how do we tweak the RAS metrics  and what would be the targets around them.  We also looked at fault handling requirements for system, PCIe, GPU, and so on.  Since then, we have been meeting every Friday, talking about how can we make further progress  and the existing requirements that we have come up with in version 1.0.  We have discussed further. We made some recent updates based on the feedbacks.  We also have looked at the performance of RAS.  We have looked at expanded system and components of RAS and debugdom that Jixin just covered.  And lastly, we have looked at the error analysis tool, which is what I'm going to talk mostly about.  All right, so before I get into that, let me just quickly touch on what is our next version scope.  So we are going to look at last-weekness reduction.  Jixin has talked about how in the training and conferencing world, how the impact can be much bigger  and how we want to be able to use our cluster more efficiently, so we will focus on that.  We will also look at confidential compute and RAS, what should be the requirements,  keeping both of them in mind.  And last but not least, we want alignments with other hardware fault management domains,  for example, RAS API.  So we had a talk this morning on RAS API, what should be the RAS interface,  how the error collection should look like, how the actions should be like in the fleet.  We want those to be part of the GPU RAS work stream as well, so that they are working together seamlessly.  All right, so with that, let's look into the error analysis tool.  So OCP is all about standardization.  It should be helpful for vendors as well as the CSPs.  So how does the error analysis tool would help us, right?  So here in this diagram, we are just showing a typical error analysis infrastructure.  The tool, error analysis tool itself, the green box in this diagram, that should come from the vendor.  So the idea is we want all the error,  all the information routed to the vendor, and the vendor would output some information.  So the standardization part here would, we are looking at it from two different points.  What is the input going into the tool, and what's the output coming out of the tool,  and how can we standardize those, okay?  So CPER, we have been talking about CPER for a long time.  That's the standard defined by UFI.  CPER common platform error records.  So as you know, in the CSP field, we can have a lot of different vendors,  and each vendor would have their own error logging infrastructure.  So we want all those error information captured in CPER format.  The CPER error record would be routed into the vendor tool.  So to be able to route those information to the right vendor tool,  we will parse the error information that will be routed to the right vendor.  That's where the green box here.  Now, the other point here is, as we have been saying,  debug done, crash done, all those should be in CPER, right?  So here, what we want is all kinds of errors, correctable, non-fatal, fatal, crash, debug,  everything should be in CPER format,  and everything should be going through the same software interface.  In some cases, to do the root cause analysis, history is helpful,  and that's where you see the other blue box there, previous CPERs.  So that's essentially the history from that vendor you would want to feed into the error analysis tool.  Then the error analysis tool will take the current CPER,  tell us the history, and then would do an analysis and come up with some recommendations.  So what sort of recommendations we are looking for?  Look, it could be simple proof of placement.  It could be, hey, you need a firmware update.  It could be that, hey, this is a debug done.  Debug done analysis is supposed to be done by the vendor,  so contact your vendor so that the information is being sent to the vendor.  Now, when the tool has done the analysis,  how the output is supposed to be.  So the first piece is the JSON output.  JSON output is supposed to be human-readable as well as machine-perceivable,  so that the CSPs are able to take the right action based on the output.  So that's what it's showing on the right-hand side.  Further analysis, dashboarding, CSPs can do that.  The other output that's showing in the bottom  is called CPADS, Common Platform Action Descriptors.  So this is another output that has been discussed in Harvard Fault Management Forum.  In this morning's talk also that they touched a little bit on this.  So the CPADS, it will go through the CSP policies.  The idea is it can directly go to the OS or to the BMC to take the action,  or it can also say,  this is an intervention of central vendor and vendor.  And for the CPADS route, we are relying on the Harvard Fault Management worksheet.  So as I said, our next version of our focus will be combining RAS API,  how it will all look like.  So in the next version, we will have it in action.  So with that, call to action.  So as Rama mentioned, for GPU RAS spec,  we have version 1.7.  For the CPU, it's .7.  Please take a look at it.  Please provide feedback.  Any questions and any updates are welcome.  And welcome to join our group.  We have the main list and the organization.  So with that, we are ready for questions.  Well, let's thank our speakers first.  I hope we have time for one question.  So if you have a question, put it in the mic and ask it.  We are self-explanatory.  No question.  Hey, thanks for that.  If microsystems would be used, right,  and especially the last year or so,  we have seen the GPUs have grown out of the box  into more of a scale of networking, right,  where it's a back level of multiple ranks  where the GPUs are going to go over and over and network, right,  or the protocol.  I think the single biggest issue that we see is back level failures here,  right, on the scale of networking, right?  I would like to see,  I would like to see how moving away from the node, right,  into more of a back level brass, right,  how does the resources be put on the broken node?  Yeah, like, there are a few discussions happened.  That's where, like, we're talking about, like,  a cluster level, how do we, like, blast areas.  And that is the next version of the thing.  And the current version, like, talks about  that, for example,  when you go scaling at the back level,  you can use different proprietary protocols or things.  Those things are covered,  but next version, like, you called out, like,  that is a very first thing we want to tackle.  That's a single biggest thing.  Yes, yes, yes.  So I think I'll be able to see what kind of...  Prioritize on that, yeah, yeah.  Thank you, yeah.  We'll prioritize that and maybe 1.9,  don't wait for 2.0.  Thank you.  I think we don't have time.  So questions, if you can get it offline.  Once again, thanks.