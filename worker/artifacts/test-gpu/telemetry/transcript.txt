 It's one of the topics that I'm interested in, because the facility's track is a piece  on the technology, and right now we're essentially looking at how workloads are changing, how  we're going to generate different pieces of data from the CPUs and show those power monitoring  systems back into the tenants' visibility for fine-tuning workloads and scheduling workloads.  So primarily I wanted to talk today about the black paper effort that we've been working  through with a number of entities from the industry.  So as many of you know, there's a collection of data points where those points come from  various entities within the infrastructure.  Chiller is a common name.  Chiller is a common name. Chiller is a common name.  Chiller is a common name.  We've got two different types of data centers.  We've got two different types of data centers, temperature sensors, pressure sensors.  All these different pieces of information are critical for what exactly the data center  is doing.  It's kind of the comparable to any machine, the entire system is within the data halls  and the data center.  The mid-time monitoring system is essential to the operations team.  So if you're looking at how the data centers are operating, that is absolutely critical.  How do you move into this AI function?  that type of information is going to become apparent to the active tenants.  It's going to work with how the supercomputers essentially operate within the data halls  is becoming more important.  This fluctuation is obviously going to be growing because of the intensities of this  backstretching.  When you think of the building of the system, you can kind of think of a couple different  ways to compartmentalize it.  The base systems within the tampa plants provide chillers, pumps, your CDUs, your secondary  fluid temperatures and pressures, the pump speeds.  All of those are tied into a kind of relative state of the infrastructure.  So when you look at a third party basin, the landlord, the building owner is responsible  for everything outside the hall.  That information needs to be readily accessible by the tenants in some shape or fashion, getting  that.  That information needs to be documented or getting a standard set in place for integration  across the different data center users is going to be very important.  Inside the data hall, of course, there's tenant type systems where you're looking at measuring  for SLA's.  You're going to essentially have NFE units, you're going to get plus weight, you're going  to get carbon meter units, you're going to get voltage, you're going to get these types  of things.  Inside the data halls, it's going to be easier because you can take those points, tie them  right back into the facility record.  That makes it quite a bit easier to work through those points.  The challenge that we're going to be facing, or are facing now, is how do we get the data  center vitals back into the tenants' visibility, and ultimately also get some of the information  from the tenants back into the landlord without duplicating the leader in building management  systems.  That has yet to be solved at scale with all the different entities.  As I mentioned previously, with the growing demand, the growing density of these racks,  fluctuations that may have been cycling on a certain period are now going to be much  greater.  The amplitudes of those peaks and valleys will change relative to traditional AI, or  traditional computer storage workloads and networking.  How does this play into the data center infrastructure?  Being able to control the infrastructure needs to be improved.  If the AI workloads can swing 20, 30 percent, if you look at the size of chillers and pumps  and these types of things, they're not really changing relative to the AI infrastructure.  How do you bring up a chiller preemptively knowing that you have a large training model  coming through?  These types of things require some sort of communication between the building systems  and the workloads themselves.  Other things need to compensate as well from the workload profiles.  Most people have probably seen the GPU inrush and the power profiles and how it's consuming  power.  That's quite a bit different than you would expect from a normal computer storage or networking  deployment.  The main piece of this furniture white paper that we've been working on is essentially  getting information from this service provider secure network back into the tenant's use  through one lead or another, which is kind of yet to be determined.  The building management system, of course, is controlling all these critical components,  so network security is extremely important.  You can imagine what would go wrong if the building management system were to be made  public on a website or whatnot.  People would be able to potentially change things like that, which would be catastrophic  to the infrastructure.  On the same token, you don't want to open up a vulnerability.  You don't want to open up a vulnerability to the tenant's private production networks,  right?  Because those are perhaps the paradox of keeping privacy and all these things.  So how do we bridge that gap to get information transported in a timely fashion with the right  reporting frequencies in these types of environments is going to be something that has to be solved  for.  Currently, today, there's a couple different secure data transmission options, although  clearly common.  Some of the reporting, so if you were to imagine, like as a person, so like QTT is a message  you hear about.  You're cueing telemetry transport, which essentially you can think of as similar to like a magazine  subscription service.  The publisher is the author of the article, or whoever owns the data initially.  The subscriber is somebody that basically wants to get that data, and the broker is  essentially like the publishing house, and they're helping you get that content.  Okay.  And of course, the topic is specific to whatever it is that the publisher has for data or information.  So that's one technique to be able to do that.  It's kind of a lightweight protocol for sharing information amongst machines.  Another good example is basically the REST API, which is fairly common for essentially  getting data and sending requests and messages through an HTTP method.  So another good example I'll learn.  The reporting of it and where does the data get stored is still really important.  So as you have started to analyze, with the waveforms, say, for a bus, or a power unit  at a bus level, being able to report the waveform for different transit modes or different modes  changing on the AI supercomputer are very important to be able to analyze.  And if you're taking snapshots of that data every one second versus 60 samples a second  or 120 samples a second, the data storage requirements are drastically different.  So a lot of this is what we're trying to make into this telemetry sub-track through the  data center facility track.  And a lot of it, like I mentioned, we've been collaborating with OCP, of course, Microsoft,  Meta, and Google.  And our whole intention is to get to a point where we can publish guidelines for third-party  storage, so that way there's a plain look upon what are the right number of building  management servers or storage devices that we need to learn to get proper data storage.  Some of the other pieces are common naming conversions.  If you've looked at building management control systems or any sort of like points lists,  you'll know that in a building you can have tens of thousands, if not hundreds of thousands  of different points.  And at the naming, most of the time, the naming scheme is kind of, it's a piece of paper.  It's a dealer's choice.  So it's whatever makes sense.  They're not really that stupid.  When you do third-party data centers, you don't want to end up having to try to tailor  point names across different tenants.  So these are some of the things that are going to be more prevalent in the future.  And so with OCP's work, we've been working on getting together the first four white papers  to explain kind of the business need, and then eventually set up these working rooms  amongst hyperscalers, you know, major types of market, third-party providers, to be able  to work on standardizing these things.  And probably one of the most important aspects of this effort is the security aspect of it.  We, many times, will find that tenants will not allow networking hardware or networking  pathways to leave the secure department.  That makes it extremely hard to get building management or power management points back  to the tenant.  If you don't allow some sort of physical transport.  So these are some of the tasks that we have to solve for.  So one of the things that I want to also call out very clearly, you know, we need folks  to be involved.  If you're a controls expert or have ideas on how we can get these transport methodologies  outlined in detail, we're always looking for support.  You can either reach out to myself or Rob Hoyle.  We've got a number of folks from...  You know, others that are also participating.  We set up a monthly call.  Although, I think as we start to subdivide some of these tasks, we'll probably set up  small working groups, especially if we get into some of the nuances of, say, pointing.  Like, we probably don't need everybody for that.  However, coming up to a standard is going to be just as straightforward.  Then, as always, link the facility...  DSM facility webpage.  If you get some questions, the persons will be reading this as well.  But that is...  Essentially, white paper is...  And then that will be out.  And then, again, the intention...  We will start to finalize that specification and implementation guide and other things  to support it to kind of go along with the change in the AI of work streams.  And one thing we need to talk about is the specific requirements, what not to run through  and how to kind of close out.  It doesn't really matter.  It's a presentation.  I think there's none that's really fine through there.  So, there's a lot of content within kind of the geometry track that we have to work through.  So, just kind of bring the awareness before that white paper comes out.  And we're looking forward to some of the standard specifications.  Awesome.  If there's no questions, thanks so much for your time.  If you have any questions, please reach out to us.  We're all cool.